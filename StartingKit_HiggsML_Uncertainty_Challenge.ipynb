{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Kit - Black Swan Detection\n",
    "\n",
    "For Overview and Decsiptions of the competition, please visit the competition page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Setup\n",
    "***\n",
    "`COLAB` determines whether this notebook is running on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Existing Submissions\n",
    "***\n",
    "By this point you should have a clone of the repo which contains `HiggsML_Dummy_Submission.zip` which you can submit to the Competition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "from numpy.random import RandomState\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Directories\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./\"\n",
    "# Input data directory to read training data from\n",
    "\n",
    "input_dir = root_dir + \"input_data/\"\n",
    "reference_dir = root_dir + 'reference_data/'\n",
    "# Output data directory to write predictions to\n",
    "output_dir = root_dir + \"sample_result_submission\"\n",
    "# Program directory\n",
    "program_dir = root_dir + \"ingestion_program\"\n",
    "# Score directory\n",
    "score_dir = root_dir + \"scoring_program\"\n",
    "# Directory to read submitted submissions from\n",
    "submission_dir = root_dir + \"sample_code_submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Add directories to path\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.append(input_dir)\n",
    "path.append(reference_dir)\n",
    "path.append(output_dir)\n",
    "path.append(program_dir)\n",
    "path.append(submission_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Internal imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization import *\n",
    "from systematics import Systematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Import Submission Model\n",
    "***\n",
    "We import a class named `Model` from the submission file (`model.py`). This `Model` class has the following methods:\n",
    "- `init`: receives train set and systematics class as input\n",
    "- `fit`: can be used for training\n",
    "- `predict`: receives one test set and outputs a dictionary with the following keys\n",
    "    - `mu_hat` : predicted mu $\\hat{\\mu}$\n",
    "    - `delta_mu_hat`: $\\Delta{\\hat{\\mu}}$ bound for $\\mu$\n",
    "    - `p16`: 16th percentile\n",
    "    - `p84`: 84th percentile\n",
    "\n",
    "In this example code, the `Model` class implements an XGBoost model which is trained to predict both the TES and the class label. You can modify it the way you want, keeping the required class structure and functions there. More instructions are given inside the `model.py` file. If running in Collab, click the folder icon in the left sidebar to open the file browser.\n",
    "\n",
    "### ⚠️ Note:\n",
    "In real setting i.e. the challenge itself, the submitted model is initialized once with train set and systematics class and the `predict` is called multiple times, each time with a different test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Data\n",
    "***\n",
    "1. Load Train set\n",
    "2. Load Test sets\n",
    "\n",
    "\n",
    "### ⚠️ Note:\n",
    "The data used here is sample data is for demonstration only to get a view of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "      self.train_set = None\n",
    "      self.test_set = None\n",
    "      self.systematics = Systematics\n",
    "      self.NUM_SETS = 4\n",
    "\n",
    "      self.test_dir = 'test' \n",
    "\n",
    "      print(\"==========================================\")\n",
    "      print(\"Data\")\n",
    "      print(\"==========================================\")\n",
    "\n",
    "  def load_train_set(self):\n",
    "    \"\"\"\n",
    "      - Loads train set\n",
    "      - Train set has no systematics\n",
    "    \"\"\"\n",
    "    print(\"[*] Loading Train set\")\n",
    "\n",
    "    train_data_file = os.path.join(input_dir, 'train', 'data', 'data.parquet')\n",
    "    train_labels_file = os.path.join(input_dir, 'train', 'labels', \"data.labels\")\n",
    "    train_settings_file = os.path.join(input_dir, 'train', 'settings', \"data.json\")\n",
    "    train_weights_file = os.path.join(input_dir, 'train', 'weights', \"data.weights\")\n",
    "\n",
    "    # read train data\n",
    "    train_data = pd.read_parquet(train_data_file, engine=\"pyarrow\")\n",
    "\n",
    "    # read train labels\n",
    "    with open(train_labels_file, \"r\") as f:\n",
    "        train_labels = np.array(f.read().splitlines(), dtype=float)\n",
    "\n",
    "    # read train settings\n",
    "    with open(train_settings_file) as f:\n",
    "        train_settings = json.load(f)\n",
    "\n",
    "    # read train weights\n",
    "    with open(train_weights_file) as f:\n",
    "        train_weights = np.array(f.read().splitlines(), dtype=float)\n",
    "\n",
    "    self.train_set = {\n",
    "        \"data\": train_data,\n",
    "        \"labels\": train_labels,\n",
    "        \"settings\": train_settings,\n",
    "        \"weights\": train_weights\n",
    "    }\n",
    "\n",
    "  def load_test_set(self):\n",
    "    \"\"\"\n",
    "      - Loads test set\n",
    "    \"\"\"\n",
    "    print(\"[*] Loading Test set\")\n",
    "    test_data_file = os.path.join(input_dir, self.test_dir, 'data', 'data.parquet')\n",
    "    test_labels_file = os.path.join(input_dir, self.test_dir, 'labels', \"data.labels\")\n",
    "    test_weights_file = os.path.join(input_dir, self.test_dir, 'weights', \"data.weights\")\n",
    "\n",
    "    # read test data\n",
    "    test_data = pd.read_parquet(test_data_file, engine=\"pyarrow\")\n",
    "\n",
    "    # read test labels\n",
    "    with open(test_labels_file, \"r\") as f:\n",
    "        test_labels = np.array(f.read().splitlines(), dtype=float)\n",
    "\n",
    "\n",
    "    # read test weights\n",
    "    with open(test_weights_file) as f:\n",
    "        test_weights = np.array(f.read().splitlines(), dtype=float)\n",
    "\n",
    "    # test settings\n",
    "    test_settings = {\n",
    "        \"ground_truth_mus\": (np.random.uniform(0.1, 3, self.NUM_SETS)).tolist()\n",
    "    }\n",
    "\n",
    "    self.test_set = {\n",
    "        \"data\": test_data,\n",
    "        \"labels\": test_labels,\n",
    "        \"settings\": test_settings,\n",
    "        \"weights\": test_weights\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilaize data\n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train set\n",
    "data.load_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test sets\n",
    "data.load_test_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Visualize\n",
    "***\n",
    "- Visualize Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualize():\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.train_visualize = Dataset_visualise(\n",
    "            data=data.train_set[\"data\"],\n",
    "            weights=data.train_set[\"weights\"],\n",
    "            labels=data.train_set[\"labels\"],\n",
    "            name=\"Train Set\"\n",
    "        )\n",
    "        print(\"==========================================\")\n",
    "        print(\"Visualize\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "\n",
    "    def show_data_summary(self):\n",
    "        \"\"\"\n",
    "            Show Dataset summary\n",
    "                - Number of events\n",
    "                - Number of features\n",
    "                - Number of signal events\n",
    "                - Number of background events\n",
    "                - mean and std of each feature and other statistical information\n",
    "        \"\"\"\n",
    "        self.train_visualize.examine_dataset()\n",
    "\n",
    "    def show_histogram(self):\n",
    "        \"\"\"\n",
    "            - plot the histogram of each feature in the dataset for signal and background events,\n",
    "            - for given set of columns. Columns to be plotted can be specified by the user\n",
    "            - This function will take a long time to run if the number of columns is large, \n",
    "            - If no columns are given it will plot all the columns of the dataset.\n",
    "        \"\"\"\n",
    "        columns = ['PRI_jet_leading_pt', 'PRI_met', 'DER_mass_vis', 'DER_mass_jet_jet', 'DER_deltar_lep_had', 'DER_sum_pt']\n",
    "        self.train_visualize.histogram_dataset(columns=columns)\n",
    "\n",
    "    def show_pair_plots(self):\n",
    "        \"\"\" \n",
    "            This function will plot the pair plots of the features in the dataset for the given columns \n",
    "            for the signal and background events separately\n",
    "\n",
    "            NB: \n",
    "            - This function will take a long time to run if the number of columns is large, \n",
    "            - If no columns are given it will plot all the columns of the dataset.\n",
    "        \"\"\"\n",
    "        columns = ['PRI_lep_pt', 'PRI_lep_eta','PRI_lep_phi', 'DER_deltar_had_lep']\n",
    "        self.train_visualize.pair_plots(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilaize Visualize\n",
    "visualize = Visualize(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "visualize.show_data_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "visualize.show_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data summary\n",
    "visualize.show_pair_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Systematics\n",
    "***\n",
    "We demonstrate how `Systematics` class can be used to add systematics to a dataset\n",
    "\n",
    "### ⚠️ Note:\n",
    "`Systematics` class is one of the input of the submitted model, Participants are free to use this class and introduce systematics in their datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Syst():\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "        self.columns = ['PRI_lep_pt', 'PRI_lep_eta','PRI_had_eta', 'DER_deltar_had_lep']\n",
    "        print(\"==========================================\")\n",
    "        print(\"Systematics\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "\n",
    "    def add_systematics_to_train_data(self):\n",
    "        print(\"[*] Adding systematics to Train set\")\n",
    "        # generate tes value uniformly between 0.9 and 1.1\n",
    "        tes = round(np.random.uniform(0.9, 1.10), 2)\n",
    "        print(f\"[*] --- tes: {tes}\")\n",
    "        # apply systematics\n",
    "        train_df_temp = self.data.train_set[\"data\"].copy()\n",
    "        train_df_temp[\"weights\"] = self.data.train_set[\"weights\"].copy()\n",
    "        train_df_temp[\"labels\"] = self.data.train_set[\"labels\"].copy()\n",
    "        train_data_with_systematics_temp = Systematics(\n",
    "            data=train_df_temp,\n",
    "            tes=tes\n",
    "        ).data\n",
    "\n",
    "        train_labels = train_data_with_systematics_temp.pop('labels')\n",
    "        train_weights = train_data_with_systematics_temp.pop('weights')\n",
    "        train_data_with_systematics = train_data_with_systematics_temp.copy()\n",
    "\n",
    "        self.train_set_with_systematics = {\n",
    "            \"data\": train_data_with_systematics,\n",
    "            \"labels\": train_labels,\n",
    "            \"weights\": train_weights,\n",
    "            \"settings\": {\"tes\": tes, \"mu\":1.0}\n",
    "        }\n",
    "\n",
    "    \n",
    "    def visualize_train_set(self):\n",
    "        train_visualize = Dataset_visualise(\n",
    "            data=data.train_set[\"data\"],\n",
    "            weights=data.train_set[\"weights\"],\n",
    "            labels=data.train_set[\"labels\"],\n",
    "            name=\"Train Set\"\n",
    "        ) \n",
    "        train_visualize.pair_plots(columns=self.columns)\n",
    "\n",
    "    def visualize_train_set_with_systematics(self):\n",
    "        train_syst_visualize = Dataset_visualise(\n",
    "            data=self.train_set_with_systematics[\"data\"],\n",
    "            weights=self.train_set_with_systematics[\"weights\"],\n",
    "            labels=self.train_set_with_systematics[\"labels\"],\n",
    "            name=\"Train Set with systematics\"\n",
    "        ) \n",
    "        train_syst_visualize.pair_plots(columns=self.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize Program\n",
    "syst = Syst(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add systematics to trian data\n",
    "systematics_train = syst.add_systematics_to_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots of orignal train set\n",
    "syst.visualize_train_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots of train set with systematics\n",
    "syst.visualize_train_set_with_systematics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Program\n",
    "***\n",
    "**`Ingestion program`** is responsible to run the submission of a participant on Codabench platform. **`Program`** is a simplified version of the **Ingestion Program** to show to participants how it runs a submission.\n",
    "1. Train a model on train data\n",
    "2. Predict using Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Program:\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        # used to keep object of Model class to run the submission\n",
    "        self.model = None\n",
    "        # object of Data class used here to get the train and test sets\n",
    "        self.data = data\n",
    "        self._dict_systematics = {\n",
    "            \"tes\": False,\n",
    "            \"jes\": False,\n",
    "            \"soft_met\": False,\n",
    "            \"w_scale\": False,\n",
    "            \"bkg_scale\": False,\n",
    "        }\n",
    "\n",
    "        # results\n",
    "        self.results = []\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Program\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "    def initialize_submission(self):\n",
    "        print(\"[*] Initializing Submmited Model\")\n",
    "        self.model = Model(train_set=self.data.train_set, systematics=Systematics)\n",
    "\n",
    "    def fit_submission(self):\n",
    "        print(\"[*] Calling fit method of submitted model\")\n",
    "        self.model.fit()\n",
    "\n",
    "    def get_bootstraped_dataset(\n",
    "        self,\n",
    "        test_set,\n",
    "        mu=1.0,\n",
    "        tes=1.0,\n",
    "        jes=1.0,\n",
    "        soft_met=1.0,\n",
    "        w_scale=None,\n",
    "        bkg_scale=None,\n",
    "        seed=0,\n",
    "    ):\n",
    "        weights = test_set[\"weights\"].copy()\n",
    "        weights[test_set[\"labels\"] == 1] = weights[test_set[\"labels\"] == 1] * mu\n",
    "        prng = RandomState(seed)\n",
    "\n",
    "        new_weights = prng.poisson(lam=weights)\n",
    "\n",
    "        del weights\n",
    "\n",
    "        temp_df = test_set[\"data\"][new_weights > 0].copy()\n",
    "        temp_df[\"weights\"] = new_weights[new_weights > 0]\n",
    "        temp_df[\"labels\"] = test_set[\"labels\"][new_weights > 0]\n",
    "\n",
    "        # Apply systematics to the sampled data\n",
    "\n",
    "        data_syst = Systematics(\n",
    "            data=temp_df,\n",
    "            tes=tes,\n",
    "            jes=jes,\n",
    "            soft_met=soft_met,\n",
    "            w_scale=w_scale,\n",
    "            bkg_scale=bkg_scale,\n",
    "        )\n",
    "\n",
    "        # Apply weight scaling factor mu to the data\n",
    "\n",
    "        data_syst.pop(\"labels\")\n",
    "        weights = data_syst.pop(\"weights\")\n",
    "\n",
    "        del temp_df\n",
    "\n",
    "        return {\"data\": data_syst, \"weights\": weights}\n",
    "\n",
    "    def predict_submission(self):\n",
    "        print(\"[*] Calling predict method of submitted model\")\n",
    "        # get set inxex = 0\n",
    "        set_indices = np.arange(0, 1)\n",
    "        # get test set indices per set (0-99)\n",
    "        test_set_indices = np.arange(0, 100)\n",
    "\n",
    "        # create a product of set and test set indices all combinations of tuples\n",
    "        all_combinations = list(product(set_indices, test_set_indices))\n",
    "\n",
    "        results_dict = {}\n",
    "        for set_index, test_set_index in all_combinations:\n",
    "\n",
    "            # random tes value (one per test set)\n",
    "            if self._dict_systematics[\"tes\"]:\n",
    "                tes = np.random.uniform(0.9, 1.1)\n",
    "            else:\n",
    "                tes = 1.0\n",
    "            if self._dict_systematics[\"jes\"]:\n",
    "                jes = np.random.uniform(0.9, 1.1)\n",
    "            else:\n",
    "                jes = 1.0\n",
    "            if self._dict_systematics[\"soft_met\"]:\n",
    "                soft_met = np.random.uniform(1.0, 5)\n",
    "            else:\n",
    "                soft_met = 1.0\n",
    "\n",
    "            if self._dict_systematics[\"w_scale\"]:\n",
    "                w_scale = np.random.uniform(0.5, 2)\n",
    "            else:\n",
    "                w_scale = None\n",
    "\n",
    "            if self._dict_systematics[\"bkg_scale\"]:\n",
    "                bkg_scale = np.random.uniform(0.5, 2)\n",
    "            else:\n",
    "                bkg_scale = None\n",
    "\n",
    "            # create a seed\n",
    "            seed = (set_index * 100) + test_set_index\n",
    "            # get mu value of set from test settings\n",
    "            set_mu = self.data.test_set[\"settings\"][\"ground_truth_mus\"][set_index]\n",
    "\n",
    "            # get bootstrapped dataset from the original test set\n",
    "            test_set = self.get_bootstraped_dataset(\n",
    "                mu=set_mu,\n",
    "                tes=tes,\n",
    "                jes=jes,\n",
    "                soft_met=soft_met,\n",
    "                w_scale=w_scale,\n",
    "                bkg_scale=bkg_scale,\n",
    "                test_set=self.data.test_set,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "            predicted_dict = self.model.predict(test_set)\n",
    "            predicted_dict[\"test_set_index\"] = test_set_index\n",
    "\n",
    "            print(\n",
    "                f\"[*] - mu_hat: {predicted_dict['mu_hat']} - delta_mu_hat: {predicted_dict['delta_mu_hat']} - p16: {predicted_dict['p16']} - p84: {predicted_dict['p84']}\"\n",
    "            )\n",
    "\n",
    "            if set_index not in results_dict:\n",
    "                results_dict[set_index] = []\n",
    "\n",
    "            results_dict[set_index].append(predicted_dict)\n",
    "\n",
    "        # prepare results for scoring\n",
    "        self.results = []\n",
    "        for i in range(0, 1):\n",
    "            set_result = results_dict[i]\n",
    "            mu_hats, delta_mu_hats, p16, p84 = [], [], [], []\n",
    "            for test_set_dict in set_result:\n",
    "                mu_hats.append(test_set_dict[\"mu_hat\"])\n",
    "                delta_mu_hats.append(test_set_dict[\"delta_mu_hat\"])\n",
    "                p16.append(test_set_dict[\"p16\"])\n",
    "                p84.append(test_set_dict[\"p84\"])\n",
    "\n",
    "            ingestion_result_dict = {\n",
    "                \"mu_hats\": mu_hats,\n",
    "                \"delta_mu_hats\": delta_mu_hats,\n",
    "                \"p16\": p16,\n",
    "                \"p84\": p84,\n",
    "            }\n",
    "            self.results.append(ingestion_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intiialize Program\n",
    "program = Program(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize submitted model\n",
    "program.initialize_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call fit method of submitted model\n",
    "program.fit_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call predict method of submitted model\n",
    "program.predict_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Score\n",
    "***\n",
    "1. Compute Scores\n",
    "2. Visualize Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Score():\n",
    "\n",
    "    def __init__(self, data, program):\n",
    "\n",
    "        self.data = data\n",
    "        self.program = program\n",
    "\n",
    "        print(\"==========================================\")\n",
    "        print(\"Score\")\n",
    "        print(\"==========================================\")\n",
    "\n",
    "    def compute_scores(self):\n",
    "        print(\"[*] Computing scores\")\n",
    "\n",
    "        # loop over ingestion results\n",
    "        self.rmses, self.maes, self.p16, self.p84, self.mus, self.mu_hats, self.intervals, self.coverages, self.quantiles_scores = [], [], [], [], [], [], [], [], []\n",
    "        all_mus, all_p16s, all_p84s = [], [], []\n",
    "        for i, (ingestion_result, mu) in enumerate(zip(self.program.results, self.data.test_set[\"settings\"][\"ground_truth_mus\"])):\n",
    "            \n",
    "            mu_hats = ingestion_result[\"mu_hats\"]\n",
    "            delta_mu_hats = ingestion_result[\"delta_mu_hats\"]\n",
    "            p16s = ingestion_result[\"p16\"]\n",
    "            p84s = ingestion_result[\"p84\"]\n",
    "\n",
    "            all_mus.extend(np.repeat(mu, len(p16s)))\n",
    "            all_p16s.extend(p16s)\n",
    "            all_p84s.extend(p84s)\n",
    "\n",
    "            set_rmses, set_maes = [], []\n",
    "            for mu_hat, delta_mu_hat in zip(mu_hats, delta_mu_hats):\n",
    "                set_rmses.append(self.RMSE_score(mu, mu_hat, delta_mu_hat))\n",
    "                set_maes.append(self.MAE_score(mu, mu_hat, delta_mu_hat))\n",
    "            set_interval, set_coverage, set_quantiles_score = self.Quantiles_Score(np.repeat(mu, len(p16s)), np.array(p16s), np.array(p84s))\n",
    "\n",
    "            set_mae = np.mean(set_maes)\n",
    "            set_rmse = np.mean(set_rmses)\n",
    "\n",
    "            print(\"------------------\")\n",
    "            print(f\"Set {i}\")\n",
    "            print(\"------------------\")\n",
    "            print(f\"MAE (avg): {set_mae}\")\n",
    "            print(f\"RMSE (avg): {set_rmse}\")\n",
    "            print(f\"Interval: {set_interval}\")\n",
    "            print(f\"Coverage: {set_coverage}\")\n",
    "            print(f\"Quantiles Score: {set_quantiles_score}\")\n",
    "\n",
    "            # Save set scores in lists\n",
    "            self.mus.append(mu)\n",
    "            self.p16.append(p16s)\n",
    "            self.p84.append(p84s)\n",
    "            self.mu_hats.append(mu_hats)\n",
    "            self.rmses.append(set_rmse)\n",
    "            self.maes.append(set_mae)\n",
    "            self.intervals.append(set_interval)\n",
    "            self.coverages.append(set_coverage)\n",
    "            self.quantiles_scores.append(set_quantiles_score)\n",
    "\n",
    "        overall_interval, overall_coverage, overall_quantiles_score = self.Quantiles_Score(np.array(all_mus), np.array(all_p16s), np.array(all_p84s))\n",
    "\n",
    "        print(\"\\n\\n==================\")\n",
    "        print(\"Overall Score\")\n",
    "        print(\"==================\")\n",
    "        print(f\"[*] --- RMSE: {round(np.mean(self.rmses), 3)}\")\n",
    "        print(f\"[*] --- MAE: {round(np.mean(self.maes), 3)}\")\n",
    "        print(f\"[*] --- Interval: {round(overall_interval, 3)}\")\n",
    "        print(f\"[*] --- Coverage: {round(overall_coverage, 3)}\")\n",
    "        print(f\"[*] --- Quantiles score: {round(overall_quantiles_score, 3)}\")\n",
    "\n",
    "    def RMSE_score(self, mu, mu_hat, delta_mu_hat):\n",
    "        \"\"\"Compute the sum of MSE and MSE2.\"\"\"\n",
    "\n",
    "        def MSE(mu, mu_hat):\n",
    "            \"\"\"Compute the mean squared error between scalar mu and vector mu_hat.\"\"\"\n",
    "            return np.mean((mu_hat - mu) ** 2)\n",
    "\n",
    "        def MSE2(mu, mu_hat, delta_mu_hat):\n",
    "            \"\"\"Compute the mean squared error between computed delta_mu = mu_hat - mu and delta_mu_hat.\"\"\"\n",
    "            adjusted_diffs = (mu_hat - mu)**2 - delta_mu_hat**2\n",
    "            return np.mean(adjusted_diffs**2)\n",
    "\n",
    "        return np.sqrt(MSE(mu, mu_hat) + MSE2(mu, mu_hat, delta_mu_hat))\n",
    "\n",
    "    def MAE_score(self, mu, mu_hat, delta_mu_hat):\n",
    "        \"\"\"Compute the sum of MAE and MAE2.\"\"\"\n",
    "\n",
    "        def MAE(mu, mu_hat):\n",
    "            \"\"\"Compute the mean absolute error between scalar mu and vector mu_hat.\"\"\"\n",
    "            return np.mean(np.abs(mu_hat - mu))\n",
    "\n",
    "        def MAE2(mu, mu_hat, delta_mu_hat):\n",
    "            \"\"\"Compute the mean absolute error based on the provided definitions.\"\"\"\n",
    "            adjusted_diffs = np.abs(mu_hat - mu) - delta_mu_hat\n",
    "            return np.mean(np.abs(adjusted_diffs))\n",
    "\n",
    "        return MAE(mu, mu_hat) + MAE2(mu, mu_hat, delta_mu_hat)\n",
    "\n",
    "    def Quantiles_Score(self, mu, p16, p84, eps=1e-3):\n",
    "\n",
    "        def Interval(p16, p84):\n",
    "            \"\"\"Compute the average of the intervals defined by vectors p16 and p84.\"\"\"\n",
    "            return np.mean(np.abs(p84 - p16))\n",
    "\n",
    "        def Coverage(mu, p16, p84):\n",
    "            \"\"\"Compute the fraction of times scalar mu is within intervals defined by vectors p16 and p84.\"\"\"\n",
    "            return_coverage = np.mean((mu >= p16) & (mu <= p84))\n",
    "            return return_coverage\n",
    "\n",
    "        def f(x, n_tries, max_coverage=1e4, one_sigma = 0.6827):\n",
    "                sigma68 = np.sqrt(((1-one_sigma)*one_sigma*n_tries))/n_tries\n",
    "\n",
    "                if (x >= one_sigma-2*sigma68 and x <= one_sigma+2*sigma68):\n",
    "                    out = 1\n",
    "                elif (x < one_sigma-2*sigma68):\n",
    "                    out = 1 + abs((x-(one_sigma-2*sigma68))/sigma68)**4\n",
    "                elif (x > one_sigma+2*sigma68):\n",
    "                    out = 1 + abs((x-(one_sigma+2*sigma68))/sigma68)**3\n",
    "                return out\n",
    "\n",
    "        coverage = Coverage(mu, p16, p84)\n",
    "        interval = Interval(p16, p84)\n",
    "        score = -np.log((interval + eps) * f(coverage, n_tries=mu.shape[0]))\n",
    "        return interval, coverage, score\n",
    "\n",
    "    def visualize_scatter(self):\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        for i, (ingestion_result, mu) in enumerate(zip(self.program.results, self.data.test_set[\"settings\"][\"ground_truth_mus\"])):\n",
    "           \n",
    "            mu_hat = np.mean(ingestion_result[\"mu_hats\"])\n",
    "            plt.scatter(mu, mu_hat, c='b', marker='o')\n",
    "        \n",
    "        plt.xlabel('Ground Truth $\\mu$')\n",
    "        plt.ylabel('Predicted $\\mu$ (averaged for 100 test sets)')\n",
    "        plt.title('Ground Truth vs. Predicted $\\mu$ Values')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_coverage(self):\n",
    "\n",
    "        fig = plt.figure( figsize=(5, 5))\n",
    "\n",
    "        for idx, (mu, mu_hats, p16s, p84s) in enumerate(zip(self.mus, self.mu_hats, self.p16, self.p84)):\n",
    "\n",
    "             # plot horizontal lines from p16 to p84\n",
    "            for i, (p16, p84) in enumerate(zip(p16s, p84s)):\n",
    "                plt.hlines(y=i, xmin=p16, xmax=p84, colors='b', label='p16-p84')\n",
    "\n",
    "            plt.vlines(x=mu, ymin=0, ymax=len(p16s), colors='r', linestyles='dashed')\n",
    "            plt.xlabel('mu')\n",
    "            plt.ylabel('psuedo-experiments')\n",
    "            plt.title(f'mu distribution - Set_{idx}')\n",
    "            \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Score\n",
    "score = Score(data=data, program=program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Score\n",
    "score.compute_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scatter plot of ground truth mu and predicted mu\n",
    "score.visualize_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coverage\n",
    "score.visualize_coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Submissions\n",
    "***\n",
    "\n",
    "### **Unit Testing**\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission.\n",
    "<br>\n",
    "Keep the sample code simple.<br>\n",
    "\n",
    "<code>python3</code> is required for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $program_dir/ingestion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Scoring Program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $score_dir/score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepare the submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "code_submission = 'HiggsML-code_submission_' + the_date + '.zip'\n",
    "zipdir(code_submission, submission_dir)\n",
    "print(\"Submit : \" + code_submission + \" to the competition\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9e001b0608738f9411416229c98988c04b997dc526fb61c5e4e084e768e3249"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
